{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f709b32f-f678-4a7b-b3eb-151a146d1e4c",
   "metadata": {},
   "source": [
    "Import the packages including jax, equinox and diffrax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "06c8e6a8-3cf2-445a-9dcc-34b3cef45bd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Union\n",
    "import pdb\n",
    "import diffrax\n",
    "import equinox as eqx  # https://github.com/patrick-kidger/equinox\n",
    "import jax\n",
    "import jax.nn as jnn\n",
    "import jax.numpy as jnp\n",
    "import jax.random as jr\n",
    "import jax.tree_util as jtu\n",
    "import matplotlib.pyplot as plt\n",
    "import optax  # https://github.com/deepmind/optax\n",
    "import numpy as np\n",
    "import control\n",
    "import pdb\n",
    "import jax\n",
    "from jax import lax\n",
    "import lineax\n",
    "from helpers_impl import generate_system,  NeuralODEPlant, get_gpu_memory\n",
    "from tqdm import tqdm\n",
    "import gc\n",
    "from IPython.display import clear_output\n",
    "from jax.flatten_util import ravel_pytree\n",
    "from diffrax import diffeqsolve, Tsit5, ODETerm, ReversibleHeun, Heun\n",
    "#Set 64 precision\n",
    "jax.config.update(\"jax_enable_x64\", True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02d7ce1e-11fe-4c18-8088-e81b0092db84",
   "metadata": {},
   "source": [
    "The block of code for initialization of Neural ODE and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f633528b-f4db-45ad-bde9-5e54d32ee498",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'A' (ndarray)\n",
      "Stored 'B' (ndarray)\n",
      "Stored 'x00' (ndarray)\n",
      "[[1 0]\n",
      " [1 0]]\n",
      "[[1]\n",
      " [0]]\n",
      "[1.  0.5]\n"
     ]
    }
   ],
   "source": [
    "#number of dimensions \n",
    "data_size=2\n",
    "\n",
    "\n",
    "#initialize the dynamical system\n",
    "A=np.array([[1,0],[1,0]])\n",
    "B=np.array([[1],[0]])\n",
    "x00=np.array([1,0.5])\n",
    "%store  A\n",
    "%store   B\n",
    "%store  x00\n",
    "print(A)\n",
    "print(B)\n",
    "print(x00)\n",
    "batch_size=1  \n",
    "\n",
    "t_size=32\n",
    "t0 = 0\n",
    "t1 = 1\n",
    "ts = jnp.linspace(t0, t1, t_size)\n",
    "tss = jnp.expand_dims(ts, axis=0)\n",
    "tsss = jnp.repeat(tss, repeats=batch_size, axis=0)\n",
    "#initialize the ODE\n",
    "seed=5679\n",
    "\n",
    "hidden_size=data_size\n",
    "noise_size=hidden_size+1\n",
    "width_size=10\n",
    "depth=5\n",
    "\n",
    "key = jr.PRNGKey(seed)\n",
    "\n",
    "generator = NeuralODEPlant(\n",
    "    data_size,\n",
    "    hidden_size,\n",
    "    width_size,\n",
    "    depth,\n",
    "    A,\n",
    "    B,\n",
    "    key=key,\n",
    ")\n",
    "\n",
    "\n",
    "#learning rate\n",
    "learning_rate=0.00001\n",
    "\n",
    "#adam optimizer\n",
    "b1=0.9\n",
    "b2=0.999\n",
    "eps=1e-08\n",
    "optim = optax.adam(learning_rate, b1=b1, b2=b2, eps=0, eps_root=eps)\n",
    "\n",
    "opt_state = optim.init(eqx.filter(generator, eqx.is_inexact_array))\n",
    "#control energy penalty\n",
    "R=0.0\n",
    "\n",
    "\n",
    "#initial conditions \n",
    "y00=jnp.append(x00, jnp.zeros((1,1)))\n",
    "y0 = jnp.expand_dims(y00, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72abee2c-f715-4901-b415-b246bf6a494c",
   "metadata": {},
   "source": [
    "Functions for evaluation of gradients of loss and control energy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "215a933b-6a0f-4b5a-b892-949e68a036a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#common denominator term\n",
    "@jax.jit\n",
    "def smoothed_inverse_grad(grad, eps=1e-05):\n",
    "    return 1.0 / jnp.sqrt(grad + eps)\n",
    "\n",
    "#for cost and its gradient evaluation\n",
    "@eqx.filter_value_and_grad  \n",
    "def loss(model):\n",
    "   \n",
    "    pred_y = jax.vmap(model)(tsss,  y0)\n",
    "    \n",
    "    los_energy = jnp.mean(jnp.sum(pred_y[:,-1,0:hidden_size]**2,axis=1))\n",
    "\n",
    "    return los_energy\n",
    "\n",
    "#for energy and its gradient evaluation\n",
    "@eqx.filter_value_and_grad  \n",
    "def loss_kin(model):\n",
    "   \n",
    "    pred_y = jax.vmap(model)(tsss,  y0)\n",
    "    \n",
    "    kin_energy=  jnp.mean(pred_y[:,-1,-1])\n",
    "\n",
    "    return kin_energy\n",
    "\n",
    "#for Hessian evaluation\n",
    "@eqx.filter_jit\n",
    "def l1(flat_params, static, unravel_fn):\n",
    "\n",
    "    unflat_params = unravel_fn(flat_params)\n",
    "    modelx = eqx.combine(static, unflat_params)\n",
    "\n",
    "    vf =  diffrax.ODETerm(modelx.vf)\n",
    "\n",
    "    solver = Heun(scan_kind=\"bounded\")\n",
    "    t0 = ts[0]\n",
    "    t1 = ts[-1]\n",
    "   \n",
    "    dt=(t1-t0)/ts.shape[0]\n",
    "    pdb.set_trace()\n",
    "    pred_y = diffeqsolve(vf, solver, t0=t0, t1=t1, dt0=dt, y0=y00, args=None)\n",
    "    \n",
    "    los_energy = jnp.mean(jnp.sum(pred_y.ys[-1,0:hidden_size]**2,axis=0))\n",
    "    score= los_energy\n",
    "\n",
    "    return score\n",
    "\n",
    "#for evaluation of directional derivatives of control energy\n",
    "@jax.jit\n",
    "def rms_full_grad(flat_grads, flat_grads1, hessian,  params, eps, ema_grad, ema_sqr_grad, counter):\n",
    "    #common denominator term\n",
    "    ema_sqr_grad = ema_sqr_grad /(1-b2**(counter+1))\n",
    "    ema_grad = ema_grad /(1-b1**(counter+1))\n",
    "    denom= smoothed_inverse_grad((ema_sqr_grad), eps)\n",
    "    dem2 = smoothed_inverse_grad((jnp.square(flat_grads)), eps)\n",
    "    \n",
    "\n",
    "    PP_ema= ema_grad * (hessian @ (ema_grad* denom) )\n",
    "\n",
    "    P_multiplier=  learning_rate* denom * ((1+b1)/(1-b1)  +((1+b2)/(1-b2))* ( eps*denom**2 -1 ))/2 \n",
    "\n",
    "\n",
    "    kin_decline_from_loss = -jnp.dot(flat_grads1, (ema_grad* denom)) \n",
    "    kin_decline_from_P_term = - jnp.dot(flat_grads1, PP_ema * P_multiplier ) \n",
    " \n",
    "\n",
    "    jax.debug.print(\"directional derivative along unmodified loss = {}\", kin_decline_from_loss)\n",
    "    jax.debug.print(\"directional derivative along bias term = {}\",  kin_decline_from_P_term)\n",
    " \n",
    "    return kin_decline_from_loss, kin_decline_from_P_term\n",
    "    \n",
    "@eqx.filter_jit\n",
    "def make_step(model, batch_size, tsss, key, opt_state, y0,  ema_grad, ema_sqr_grad, counter):\n",
    "\n",
    "    key = jr.split(key, batch_size)\n",
    "    \n",
    "    #evaluate loss, energy and gradients\n",
    "    value, grads = loss(model)\n",
    "    value1, grads1 = loss_kin(model)\n",
    "    #hessian evaluation\n",
    "    arr, static = eqx.partition(model, eqx.is_inexact_array)\n",
    "    params, unravel_fn = ravel_pytree(arr)\n",
    "    hessian=jax.hessian(l1)(params, static, unravel_fn)\n",
    " \n",
    "    #flattening the gradients\n",
    "    flat_grads, _ = ravel_pytree(eqx.filter(grads, eqx.is_inexact_array))\n",
    "    flat_grads1, _ = ravel_pytree(eqx.filter(grads1, eqx.is_inexact_array))\n",
    "\n",
    "    #evaluation of ema gradients\n",
    "    ema_grad = b1*ema_grad + (1-b1) * flat_grads\n",
    "    ema_sqr_grad = b2*ema_sqr_grad + (1-b2) * jnp.square(flat_grads)\n",
    "\n",
    "    cos_theta_grad_loss, cos_theta_grad_kin=rms_full_grad(flat_grads, flat_grads1, hessian, params, eps, ema_grad, ema_sqr_grad, counter)\n",
    "\n",
    "    #update the control network\n",
    "    updates, opt_state = optim.update(grads, opt_state)\n",
    "    model = eqx.apply_updates(model, updates)\n",
    "\n",
    "    counter+=1\n",
    "    return model, opt_state, grads, value, value1, cos_theta_grad_kin, cos_theta_grad_loss, jnp.linalg.norm(params), ema_grad, ema_sqr_grad, counter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddf20753-5410-4ba3-ae14-535251e9a746",
   "metadata": {},
   "source": [
    "The loop for evaluation of control policy, loss and control energy, directional derivatives "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d2eae21a-f306-4371-a650-9a6b4dbee923",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> \u001b[0;32m/tmp/ipykernel_1820067/3142518197.py\u001b[0m(99)\u001b[0;36ml1\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m     97 \u001b[0;31m    \u001b[0mdt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt1\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mt0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     98 \u001b[0;31m    \u001b[0mpdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m---> 99 \u001b[0;31m    \u001b[0mpred_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdiffeqsolve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msolver\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt0\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mt0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mt1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdt0\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my0\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my00\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    100 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    101 \u001b[0;31m    \u001b[0mlos_energy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred_y\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mys\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mhidden_size\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  vf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ODETerm(\n",
      "  vector_field=VectorField(\n",
      "    poly0=MLP(\n",
      "      layers=(\n",
      "        Linear(\n",
      "          weight=f32[10,1],\n",
      "          bias=f32[10],\n",
      "          in_features=1,\n",
      "          out_features=10,\n",
      "          use_bias=True\n",
      "        ),\n",
      "        Linear(\n",
      "          weight=f32[10,10],\n",
      "          bias=f32[10],\n",
      "          in_features=10,\n",
      "          out_features=10,\n",
      "          use_bias=True\n",
      "        ),\n",
      "        Linear(\n",
      "          weight=f32[10,10],\n",
      "          bias=f32[10],\n",
      "          in_features=10,\n",
      "          out_features=10,\n",
      "          use_bias=True\n",
      "        ),\n",
      "        Linear(\n",
      "          weight=f32[10,10],\n",
      "          bias=f32[10],\n",
      "          in_features=10,\n",
      "          out_features=10,\n",
      "          use_bias=True\n",
      "        ),\n",
      "        Linear(\n",
      "          weight=f32[10,10],\n",
      "          bias=f32[10],\n",
      "          in_features=10,\n",
      "          out_features=10,\n",
      "          use_bias=True\n",
      "        ),\n",
      "        Linear(\n",
      "          weight=f32[1,10],\n",
      "          bias=f32[1],\n",
      "          in_features=10,\n",
      "          out_features=1,\n",
      "          use_bias=True\n",
      "        )\n",
      "      ),\n",
      "      activation=<wrapped function relu>,\n",
      "      final_activation=<function <lambda>>,\n",
      "      use_bias=True,\n",
      "      use_final_bias=True,\n",
      "      in_size=1,\n",
      "      out_size=1,\n",
      "      width_size=10,\n",
      "      depth=5\n",
      "    ),\n",
      "    A=i32[2,2],\n",
      "    B=i32[2,1],\n",
      "    hidden_size=2\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  y00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Array([1. , 0.5, 0. ], dtype=float32)\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  exit\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100000 [11:38<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "#compute the first values of cost and energy for ema \n",
    "tot_iter=100000\n",
    "losses, grads = loss(generator)\n",
    "kin, grads1 = loss_kin(generator)\n",
    "arr, static = eqx.partition(generator, eqx.is_inexact_array)\n",
    "params, unravel_fn = ravel_pytree(arr)\n",
    "flat_grads, _ = ravel_pytree(eqx.filter(grads, eqx.is_inexact_array))\n",
    "ema_grad =  flat_grads\n",
    "ema_sqr_grad = jnp.square(flat_grads)\n",
    "\n",
    "\n",
    "#save costs and energy\n",
    "saved_loss=[]\n",
    "saved_kin=[]\n",
    "#save directional derivatives\n",
    "saved_product1=[]\n",
    "saved_product2=[]\n",
    "\n",
    "ini_key=jr.PRNGKey(seed)\n",
    "key= jr.split(key, 1)[0]\n",
    "alpha=0.9\n",
    "\n",
    "counter=jnp.int32(0)\n",
    "tsss = jnp.repeat(tss, repeats=batch_size, axis=0)\n",
    "\n",
    "score=10000000\n",
    "epochs=[]\n",
    "ema_loss=0\n",
    "for k in tqdm(range(0,tot_iter)):\n",
    "   \n",
    "    key= jr.split(key, 1)[0]\n",
    "    #run optimization steps, evaluate the cost and energy, directional derivatives\n",
    "    generator, opt_state, grads, losses, kin, cos_theta_imp, cos_theta_loss, param_norm, ema_grad, ema_sqr_grad, counter= make_step(generator, batch_size, tsss, key, opt_state, y0,  ema_grad, ema_sqr_grad,counter)\n",
    "    \n",
    "    if k==0:\n",
    "        ema_loss=losses\n",
    "    else:\n",
    "        ema_loss=ema_loss*alpha+losses*(1-alpha)\n",
    "    \n",
    "    if ema_loss<score:\n",
    "        score=ema_loss\n",
    "\n",
    "    print(\"ema loss\")\n",
    "    print(ema_loss)\n",
    "    print(\"kin\")\n",
    "    print(kin)\n",
    "    saved_loss.append(jax.device_get(losses))\n",
    "    saved_kin.append(jax.device_get(kin))\n",
    "    saved_product1.append(jax.device_get(cos_theta_imp))\n",
    "    saved_product2.append(jax.device_get(cos_theta_loss))\n",
    "\n",
    "    if k%500==0:\n",
    "        gc.collect()\n",
    "        clear_output(wait=True)\n",
    "        \n",
    "    if jnp.linalg.norm(ema_loss)<1e-06:\n",
    "        break\n",
    "    del grads, losses, kin\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
